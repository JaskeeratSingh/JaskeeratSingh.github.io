“A Convolutional Approach to using AI to help the Speech-Impaired communicate”
There are approximately 18.5 million people globally with speech-disorders. The objective is to develop a model that learns the relationship between biological input and speech. The idea here is to break the problem of speech into a binary problem and then train a model to be able to pick up on any biological binary input and remap it to speech data as output. Consider Morse-code, a 'dit' and a 'dah', 2 signals which are enough to convey any text. The end result is to be able to train a user to perform a biological binary operation in Morse-code, which the model deciphers and then conveniently converts to text. Practically, consider an EEG (Electroencephalogram) and choose a simple binary operation, E.g. an eye blink(Short and Long blink) Pass the EEG output as an input to a deep learning system to learn the relationship between short and long blinks and the EEG output. At this stage, the model can classify short and long blinks based on just brain waves. We now train the user to learn and blink in Morse code. Initially, this would be exhausting; as Morse-Code works letter by letter, this will be very slow. However, the beauty of the Human Brain is that it possesses the property of Neuro-Plasticity, so once the user gets comfortable over a week with Morse-code blinking patterns, they won't have to perform it or blink physically. Just by merely thinking about the blink patterns, the same neurons light up which results in the same speech output from the model. This property suddenly takes this process from frustratingly slow to the speed of regular speech.
The idea’s spark came from reading a paper from Rajit Manohar(Faculty-Yale) and discussion with Kavita Bala (Dean,Computer-Science,CornellUniversity). 
-Shortlisted among Top 40 researchers among India under 18.
